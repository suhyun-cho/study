{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache-Hadoop\n",
    "T-academy 온라인강의 <br>\n",
    "https://tacademy.skplanet.com/live/player/onlineLectureDetail.action?seq=188\n",
    "\n",
    "## [3강] 하둡 분산파일시스템 이해(1)\n",
    "수강일 : 2020-11-25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------\n",
    "#### 분산파일 시스템(HDFS)\n",
    "* 여러대의 서버가 하나처럼 동작하는 구조.\n",
    "* 구글 파일시스템(GFS)의 아키텍쳐 기반으로 만들어짐\n",
    "* 마스터와 client는 데이터를 주고받지 않고, chunkserver와 데이터를 주고받게 만들어짐.\n",
    "    * 마스터 슬레이브 구조의 플랫폼의 특징임.\n",
    "    * 마스터에 장애가나면 전체 클러스터에 문제가 생기기 때문에, 마스터의 안정성을 보장하기위해서\n",
    "\n",
    "#### 구글 플랫폼의 철학\n",
    "1. 한대의 고가 장비보다 여러 대의 저가 장비가 낫다 (Scale-out)\n",
    "2. 데이터는 분산 저장한다. (Distributed computing)\n",
    "3. 시스템(H/W)은 언제든 죽을 수 있다(Smart S/W)\n",
    "4. 시스템 확장이 쉬워야 한다.\n",
    "    * ex) 데이터가 늘어났을 때, 스펙을 올리고 개발을 새로하는게아니라 장비를 추가해서 클러스터 노드 수를 늘리면 원래처럼 잘 동작해야한다.\n",
    "    \n",
    "#### 하둡 특성\n",
    "* 수천대 이상의 리눅스 기반 범용 서버들을 하나의 클러스터로 사용\n",
    "* 마스터-슬레이브 구조\n",
    "* 파일은 블록(block) 단위로 저장\n",
    "* 블록 데이터의 복제본 유지로 인한 신뢰성 보장\n",
    "* 높은 내고장성\n",
    "* 데이터 처리의 지역성 보장\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------\n",
    "#### 하둡 클러스터 네트워크 및 데몬 구성\n",
    "> * Name Node : 하둡 분산 파일 시스템에 대한 마스터 데몬 (데이터의 위치, 형식 보관)\n",
    "    * DN : DataNode (실 데이터 저장)\n",
    "    * TT : TaskTracker (어플리케이션을 수행하는 역할)\n",
    "    * DN,TT는 보통 세트이고 \n",
    "* Job Tracker\n",
    "    * DN + TT 두개 같이 띄움\n",
    "* Secondary NN\n",
    "* Client\n",
    "\n",
    "#### 하둡에서 블록(Block)이란 ?\n",
    "* 만약 612MB짜리 문서를 저장한다고할때, 128MB, 128MB, 128MB,,,식으로 쪼개서 저장함.\n",
    "    * 128MB 저장하게끔 만들어져 있는것\n",
    "    \n",
    "#### 하둡에서 블록(Block) 하나의 크기가 큰 이유는 ?\n",
    "* HDFS의 블록은 128MB와 같이 매우 큰 단위\n",
    "* 블록이 큰 이유는 탐색 비용을 최소화할 수 있기 때문에\n",
    "* 블록이 크면 하드디스크에서 블록의 시작점을 탐색하는데 걸리는 시간을 줄일 수 있고, 네트워크를 통해 데이터를 전송하는 데 더 많은 시간을 할당\n",
    "\n",
    "#### 하둡 저장 특징\n",
    "1. 데이터를 조각내어 서버내 분산 저장\n",
    "2. 데이터를 복사하여 여러 개를 저장\n",
    "\n",
    "* Name node는 하둡 전체 클러스터에 저장되어있는 모든 블록이 어느 서버에 카피되어있는지 다 알고있는 상태. \n",
    "* 만약 한대의 서버에 장애가나더라도, Name node 명령하에 Data node끼리 데이터를 다시 카피해놓음.\n",
    "* 하둡 플랫폼에서 자동으로 동작하므로 운영자가 액션을 취할 것은 없음.\n",
    "* 하둡이 데이터 유실이될 수 있는 경우 ?\n",
    "    * replication이 3개인데, 3개가 동시에 장애가 날 경우\n",
    "    * 하둡 플랫폼 자체에 문제가있어서 데이터가 유실되는 경우는 거의 없다고 보면 됨.\n",
    "    \n",
    "Q : 마스터노드가 고장나면 대비책은 ? <br>\n",
    "A : 하둡 2.0은 마스터노드의 고장에 대한 대비책이 있음. 마스터 서버의 이중화.<br>\n",
    "2.0 에서 3.0으로 갈때의 변화는 replication의 두배정도 저장이됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------\n",
    "#### 블록의 지역성 (Locality)\n",
    "* 네트워크를 이용한 데이터 전송 시간 감소\n",
    "* 대용량 데이터 확인을 위한 디스크 탐색 시간 감소\n",
    "* 적절한 단위의 블록 크기를 이용한 CPU 처리시간 증가\n",
    "> 요약 : 데이터가 있는 곳에 가서 먼저 실행을 하는 것 \n",
    "\n",
    "#### 블록 캐싱\n",
    "* 데이터 노드에 저장된 데이터 중 자주 읽는 블록은 블록캐시라는 데이터 노드의 메모리에 명시적으로 캐싱할 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------\n",
    "#### 보조 네임노드(SNN)\n",
    "* 네임노드(NN)와 보조 네임노드(SNN)\n",
    "    * fsimage와 edits 파일을 주기적으로 병합\n",
    "* 체크 포인트\n",
    "    * edits로그가 일정 사이즈 이상이면 실행\n",
    "    \n",
    "#### 데이터노드(Datanode)역할\n",
    "* 데이터노드는 물리적으로 로컬 파일시스템에 HDFS데이터를 저장\n",
    "* 일반적으로 레이드 구성을 하지 않음 (JBOD구성)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "suhyun2",
   "language": "python",
   "name": "suhyun2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
