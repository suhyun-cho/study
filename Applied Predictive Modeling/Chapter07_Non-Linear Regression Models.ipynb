{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실전 예측 분석 모델링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7장. 비선형 회귀 모델**\n",
    "신경망 모델, 다변량 적응 회귀 스플라인(multivariate adaptive regression splines, MARS), 서포트 벡터 머신(SVMs), KNN <br>\n",
    "\n",
    "### 7.1 신경망 모델\n",
    "신경망은 뇌의 작동 방식에 대한 이론에서 영향을 받은 강력한 비선형 회귀 기법이다. <br>\n",
    "* 각 은닉 단위는 일부 또는 전 예측 변수의 선형 조합이다.\n",
    "\n",
    "* 이런 선형 조합은 일반적으로 비선형 함수로 로지스틱(시크모이드 함수 등) 함수 같은 것이 사용된다.\n",
    "\n",
    "* 또한 신경망은 회귀 계수가 커짐에 따라 예측 변수와 응답 변수 간의 관계를 과적합시키는 경향이 있다.\n",
    "\n",
    "* 인수가 많은 경우에 선택된 모델은 지역 최적화된 인수 추정값을 찾게 된다. 알고리즘상에서는 수렴하지만, 그 결과로 나온 인수 추정값은 전역 최적 추정값이 아닌 것이다 .\n",
    "\n",
    "    * 이에 대한 대안으로 안정적인 예측값을 생성하기 위해 서로 다른 시작값을 만든 후, 이에 대한 결과의 평균을 내는 방식을 사용한다. (모델 평균 방식) \n",
    "    \n",
    "        * 하지만 이는 때때로 예측 변수들간의 높은 상관성을 만들어 내기도 하는데, 이를 완화하기 위해서는 상관관계가 있는 변수들을 사전에 거르거나, <br>\n",
    "        주성분 분석같은 특징 추출 기법을 모델링 전에 사용하는 것이 있다.\n",
    "        \n",
    "\n",
    "#### **과적합을 줄이는 방법**\n",
    "* 가중값 감소(weight decay) 를 사용하는 벌점 방식\n",
    "    * 능형 회귀(Ridge regression) 처럼 모델을 정규화하는 것.\n",
    "\n",
    "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "### 7.2 다변량 가법 회귀 스플라인 모델 (MARS)\n",
    "* PLS와 신경망은 예측 변수의 선형 조합 기반인데 반해, MARS는 모델에 사용하는 예측 변수를 두 가지 상반된 형태로 만든다.\n",
    "\n",
    "* 또한 MARS에서 원변수 대신 사용되는 요소는 보통 한 번에 예측 변수 한두 개가 사용되는 함수다.\n",
    "\n",
    "* MARS의 요소들은 예측 변수를 두 그룹으로 나누어 각 그룹에 대해 예측 변수와 결과값의 선형 조합을 모델링하는 특성을 갖고 있다.\n",
    "\n",
    "* 모델에서 각 요소의 기여도를 판단할 때는 GCV 통계량을 사용한다. \n",
    "\n",
    "* 요약하면, MARS모델에 관련해서는 2개의 튜닝 변수가 있다. 모델에 더해지는 요소의 차수와 남아있는 항의 차수다.\n",
    "\n",
    "* 후자의 경우(GCV를 활용하는) 기본 가지치기 과정이나 사용자의 설정을 통해 자동으로 정해지거나, 리샘플링 기법을 활용함으로써 결정할 수 있다.\n",
    "    * 확실히 GCV추정값이 교차 검증 과정이나 테스트 세트를 사용해 구한 값보다 긍적적으로 나타나는 것을 확인.\n",
    "    * 하지만 MARS상의 내부 GCV추정값은 단일 모델에 대해서만 값을 구하지만, <br>\n",
    "    외부 교차 검증 과정은 특징 선택을 포함한 전체 모델 구축 과정 모두를 대표한다는 것을 염두에 두자. <br>\n",
    "    \n",
    "    \n",
    "* GCV 추정값은 특징 선택상에서 야기될 수 있는 *편향적 선택*으로 인한 불확실성을 반영하지 않는다.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### **MARS 사용시 이점**\n",
    "1) 모델에서 자동으로 특징 선택을 실행한다. \n",
    "    * 이를 통해 모델 방정식은 최종 모델에 들어갈 어떤 요소에도 포함되지 않는 예측 변수에 대해 독립적이다. \n",
    "2) 해석력이 있다.\n",
    "    * MARS모델은 가법 모형이므로 각 예측 변수는 다른 변수를 고려할 필요없이 각각 단독으로 기여한다.\n",
    "    * 이를 통해 어떻게 각 예측 변수가 결과값과 연관되는지를 명확하게 해석할 수 있다.\n",
    "3) MARS모델은 데이터를 전처리할 필요가 거의 없다. \n",
    "    * 한 예로, 영분산 예측 변수는 예측에 필요한 정보가 전혀 없으므로 예측 공간을 양분할 때 애초에 선택되지 않는다.\n",
    "<br>\n",
    "\n",
    "#### **회귀 스플라인과 다항식회귀 비교**\n",
    "* 다항식은 유연한 적합을 위해 높은 차수를 사용해야 하지만, <br>\n",
    "    스플라인은 차수는 고정시키고 매듭의 수를 증가시켜 유연성을 높일 수 있기 때문에 월등히 좋은 결과를 준다.\n",
    "\n",
    "* 회귀 스플라인은 X를 조각 낸 후에 다항식회귀를 수행하는데, 기존에 다항식회귀에서 높은 차수를 사용하여 생기는 Overfitting 문제를 <br>\n",
    "    회귀 스플라인에서는 X의 범위를 조각내어 하나의 수식에 의존하는 것이 아닌,<br>\n",
    "    각 X의 범위 마다 다항식을 적용하여 구간별로 적합하기 때문에 Overfitting의 문제를 줄일 수 있다.\n",
    "\n",
    "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "### 7.3 서포트 벡터 머신(Support Vector Machine)\n",
    "SVM은 매우 강력하고 유연성 높은 모델링 기법 유형이다.(내재된 이론은 분류 모델용으로 만들어진 것) <br>\n",
    "\n",
    "* SVM에서 중요한 요소 3가지는 마진(Margin), 서포트벡터(Support Vector), 커널(Kernel)이다.\n",
    "\n",
    "**1.1 마진** <br>\n",
    "마진은 하나의 데이터포인트(Support Vector)와 판별경계(Hyperplane)사이의 거리를 말한다. <br>\n",
    "SVM에서는 이 마진이 클수록 분별을 잘하는 분류기인데 분류기와 데이터포인트간의 거리(마진)를 두는 이유는 <br>\n",
    "학습오차와 일반화오차 중 일반화오차 부분 즉, 데이터속에서 내제하는 기본적인 오차부분정도(노이즈)를 표현하기 위해 존재한다. <br>\n",
    "* 마진은 여러개의 판별경계후보가 있을 때 벡터공간에서의 가장 합리적인 판별경계를 찾을 수 있는 기준인 셈이다. <br>\n",
    "\n",
    "**1.2 서포트벡터 (Support Vector)** <br>\n",
    "서포트 벡터는 위에서 말한 데이터 포인트가 바로 서포트 벡터이다. <br>\n",
    "즉, 판별경계까지의 거리가 가장 짧은 데이터 벡터를 서포트 벡터라고 한다. <br>\n",
    "* $\\alpha$ 가 0이아닌 데이터 포인트. 이 샘플을 사용해 만들어진 회귀선을 서포트 벡터라고 부름.\n",
    "* 이 서포트벡터로 인해 SVM이 가지는 장점은, 새로운 데이터포인트가 들어왔을 때 전체 데이터포인트와의 내적거리를 보지 않고 ,br>\n",
    "    서포트벡터와의 내적거리만 구하면 되므로 상당히 계산비용을 줄일 수 있다.\n",
    "\n",
    "**1.3 커널(Kernel)** <br>\n",
    "선형분리가 불가능한 저차원의 데이터를 고차원의 공간의 값으로 매핑시켜 선형평면으로 분류 가능한 선형문제로 변환을 시켜 분류를 가능하게 할 수 있지만, <br>\n",
    "여기서 차원을 높임으로 인해 계산비용이 높아지는 문제가 생긴다. <br>\n",
    "이러한 문제를 해결할 수 있는 방법이 바로 커널법이다. <br>\n",
    "\n",
    "각 특징벡터의 고차원 매핑함수 f()를 각각 정의하지 않고, 전체의 내적함수만 정의하여 계산량을 줄일 수 있다. <br>\n",
    "여기서 나온 이 내적함수가 바로 커널함수이고, 이렇게 계산량을 줄이는 방법을 커널법 또는 커널트릭이라고 한다 .<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "**[서포트 벡터 머신의 장단점]**\n",
    "* 장점 \n",
    "    * 분류문제나 예측문제 동시에 쓸 수 있다.\n",
    "    * 신경망 기법에 비해 과적합 정도가 덜하다.\n",
    "    * 예측의 정확도가 높음\n",
    "    * 사용하기 쉽다\n",
    "    \n",
    "* 단점\n",
    "    * Kernel과 모델 파라미터를 조절하기 위한 테스트를 여러번 해봐야 최적화된 모형을 만들 수 있다.\n",
    "    * 모형 구축 시간이 오래걸린다\n",
    "    * 결과에 대한 설명력이 떨어진다.\n",
    "\n",
    "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "### 7.4 K-최근접 이웃\n",
    "KNN방식은 단순히 훈련 세트에서 K개의 가장 근접한 샘플을 사용해 새로운 샘플에 대해 예측한다. <br>\n",
    "* KNN방법은 사용자가 샘플 간의 거리를 어떻게 정희하느냐에 따라 달라진다. *유클리드 거리 (두 샘플 간의 직선거리)* 가 일반적으로 사용된다. <br>\n",
    "    * '민코프스키 거리' 는 유클리드 거리를 일반화한 것. (q=2일때 민코프스키 거리와 유클리드 거리가 동일)\n",
    "\n",
    "* KNN기법은 기본적으로 샘플 간의 거리를 기반으로 하므로 예측 변수의 척도가 샘플 간 거리에 엄청난 영향을 미친다.\n",
    "    * 큰 척도의 예측변수에 가장 많은 무게가 쏠리므로, KNN을 실행하기 전 예측 변수에 대해 중심화 및 척ㄱ도화하는 것을 추천.\n",
    "\n",
    "**[샘플 간 거리를 사용할 때, 샘플상에서 하나 이상의 예측 변숫값이 누락된 경우]**\n",
    "* 해당 샘플이나 해당 예측변수를 분석에서 제거\n",
    "* 예측 변수가 전 샘플에 걸쳐 충분한 양의 정보를 담고 있다면, 결측값을 대치하는 방법\n",
    "\n",
    "**[KNN 특징]**\n",
    "* K가 작은 경우에는 과적합 현상을 보이고, K가 큰 경우에는 과소적합 현상을 보임. \n",
    "    * ex) k=4일때 RMSE값이 가장 작고, K가 커짐에 따라 RMSE값 증가.\n",
    "* 단점 \n",
    "    * 샘플 예측에 있어서 한 샘플과 다른 모든 샘플 간의 거리를 계산해야 하므로 시간이 많이 소요됨.\n",
    "        * 대안 : k-차원 트리(k-d트리)\n",
    "\n",
    "(참고) <br>\n",
    "* 분류 (Classification) \n",
    "    * 소속집단의 정보를 이미 알고 있는 상태에서, 비슷한 집단으로 묶는 방법. (지도학습 Supervised Learning)\n",
    "    * 위의 KNN은 '분류'에 속함.\n",
    "* 군집화 (Clustering)\n",
    "    * 소속집단의 정보가 없고, 모르는 상태에서 비슷한 집단으로 묶는 방법. (비지도학습 Unsupervised Learning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "suhyun2",
   "language": "python",
   "name": "suhyun2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
