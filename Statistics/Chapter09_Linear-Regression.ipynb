{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9장. 회귀분석\n",
    "* 9.1 단순선형회귀\n",
    "* 9.2 다중선형회귀\n",
    "* 9.3 회귀진단\n",
    "\n",
    "참고[1] : https://godongyoung.github.io/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/2018/01/20/ISL-linear-regression_ch3.html   <br>\n",
    "참고 [2] : http://hosting03.snu.ac.kr/~hokim/int/2017/chap_91.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 단순선형회귀\n",
    "### 9.1.1 회귀모형 결정\n",
    "단순선형회귀는 반응변수(종속변수)와 설명변수(독립변수)의 관계가 선형이고 설명변수의 개수가 하나인 회귀분석이다.\n",
    "\n",
    "부모의 키를 X, 성인 자녀의 키를 Y라고 할 때, X의 값을 고정하면 몇 개의 특이점들을 제외하면 Y가 전체적으로 정규분포와 비슷한 형태를 띈다.<br>\n",
    "또, X의 값이 커질 때 Y의 분포의 중심이 X에 대해 선형으로 커지고, X의 값이 달라지더라도 Y의 분산은 거의 비슷하다는 것을 볼 수 있다. <br>\n",
    "\n",
    "이를 바탕으로 X의 값이 x로 주어졌을 때, Y의 기댓값은 x에 대해 선형꼴이며 분산은 x값에 대해 상수인 정규분포를 따른다는 가정을 할 수 있다. <br>\n",
    "위와 같은 사실들로부터 단순선형회귀모형  <br>\n",
    "### $Y=\\beta_0 + \\beta_1 x + \\epsilon$ ~ $N(0,\\sigma^2)$ \n",
    "을 고려할 수 있다. <br>\n",
    "\n",
    "이에 수반되는 가정은 다음과 같다. <br>\n",
    "i) $\\beta_0$ 와 $\\beta_1$은 직선의 절편과 기울기에 해당하는 회귀모수이다.(회귀계수 라고도 함) <br>\n",
    "ii) 오차항 $\\epsilon_i$ 들은 서로 독립이며 $N(0,\\sigma^2)$ 분포를 따른다.\n",
    "\n",
    "#### (+) 추가 \n",
    "* 좋은 회귀분석 모형이란, 종속변수가 평균값으로부터 다른 정도(변동)를 잘 설명할 수 있는 독립변수로 구성된 모형이라 할 수 있다.\n",
    "\n",
    "### 9.1.2 회귀모형에 대한 추론\n",
    "두 변수 X와 Y에 대한 단순선형회귀모형 $Y=\\beta_0 + \\beta_1 x + \\epsilon$을 적합할 때,  <br>\n",
    "$\\beta_0, \\beta_1, \\sigma^2$ 은 미지의 모수이므로 자료를 사용하여 추정하여야 한다. <br>\n",
    "$\\beta_0, \\beta_1$은 X와 Y의 직선관계를 결정하는 계수들이므로, 산점도에 나타난 자료를 가장 잘 설명하는 직선을 골라 그 직선의 절편과 기울기로 각각 추정할 수 있을 것이다. <br>\n",
    "\n",
    "어떤 직선이 자료에 가장 '가까운'가는 산점도 위에 직선을 그린다면, 자료와 직선 간의 수직거리를 기준으로 삼아 이를 최소화하는 것이 타당한 방법이 될 수 있다. <br>\n",
    "\n",
    "산점도에서 자료와 직선 간의 수직거리는 i번째 개체에 대하여 $Y_i - (\\beta_0 + \\beta_1 x_i)$ 가 된다. <br>\n",
    "이는 실제로 관측된 반응변수의 값 $Y_i$와 x와의 선형 관계를 통해 예측한 반응변수의 값 $\\beta_0 + \\beta_1 x_i$와의 차이이며 <br>\n",
    "이 편차의 제곱을 모든 개체들에 대하여 더한 편차제곱합을 추정의 기준으로 삼을 수 있다. <br>\n",
    "### $\\sum_{i=1}^n (Y_i-(\\beta_0 + \\beta_1 x_i))^2$ \n",
    "으르 최소화하는 $\\beta_0$ 와 $\\beta_1$을 구하여 이를 $\\hat \\beta_0$와 $\\hat \\beta_1$으로 표기하고 회귀계수에 대한 **최소제곱추정량** 이라고 한다.\n",
    "\n",
    "#### **[최소제곱법]**\n",
    "근사적으로 구하려는 해와 실제 해의 오차의 제곱의 합이 최소가 되는 해를 구하는 방법. <br>\n",
    "\n",
    "\n",
    "#### **[최소제곱 추정량 $\\hat \\beta_0$ 과 $\\hat \\beta_1$ 구하는 법]**\n",
    "편차 제곱합  $\\sum_{i=1}^n (Y_i-(\\beta_0 + \\beta_1 x_i))^2$  을 각각 $\\beta_0$ 와 $\\beta_1$ 에 대해 미분한 식을 0으로 놓고, $\\beta_0$ 와 $\\beta_1$에 대해 풀어 구하면 , <br>\n",
    "### $\\hat \\beta_1 = \\frac{\\sum_{i=1}^n (x_i-\\bar x_n)(Y_i-\\bar Y_n)}{\\sum_{i=1}^n (x_i - \\bar x_n)^2}$\n",
    "### $\\hat \\beta_0 = \\bar Y_n - \\hat \\beta_1 \\bar x_n$\n",
    "가 편차제곱합을 최소로 만든다는 것을 확인할 수 있다 <br>\n",
    "\n",
    "이렇게 구한 최소제곱추정량 $\\hat \\beta_0$과 $\\hat \\beta_1$을 통해 추정된 회귀직선 $\\hat Y = \\hat \\beta_0 + \\hat \\beta_1 x$ 를 사용하면 i번째 개체의 반응변수의 값을 <br>\n",
    "$\\hat Y_i = \\hat \\beta_0 + \\hat \\beta_1 x_i$로 예측할 수 있다. <br>\n",
    "이때 실제 관측값 $Y_i$와 예측값 $\\hat Y_i$의 차이인 $Y_i - \\hat Y_i$ 을 **잔차**(residual)라고 한다.\n",
    "\n",
    "#### **[회귀 직선의 기울기]** <br>\n",
    "회귀 직선의 기울기 $\\beta_1$은 설명변수 x의 값을 한 단위 증가시킬 때 Y의 값의 변화에 대한 측도이다. <br>\n",
    "만약 회귀직선이 $Y= 2+3x$라면, x를 한 단위 증가시킬 때마다 Y는 세 단위가 증가한다는 뜻이다. <br>\n",
    "\n",
    "* 회귀직선의 기울기는 반응변수와 설명변수의 표준편차에도 영향을 받으므로, 기울기가 크다고 해서 두 변수의 선형관계가 강하다고 할 수는 없다. \n",
    "* 회귀직선의 절편은 x=0 일때의 Y값이므로, 직선을 적합할 때 꼭 필요하지만 x가 0 또는 그 주변의 값을 가질 수 없는 경우에는 그 자체로 특별히 의미 있는 값이 되지 못한다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#### **[모수 $\\sigma^2$ 의 추정]**\n",
    "$\\hat \\beta_0$ 와 $\\hat \\beta_1$ 추정을 통해 회귀선을 추정했는데,<br>\n",
    "그럼 이제 궁금한것은 '얼마나 기대값이 믿을만한가' 이다. <br>\n",
    "이는 추정된 값 ($\\hat y_0, \\hat \\beta_0, \\hat \\beta_1$) 의 **분산**을 구하는 문제가 된다. (기대값이 같아도 분산은 천차만별일 수 있다.) <br>\n",
    "* 이때, 이 '분산' 역시 진짜 값은 알 수 없고 우리가 가진 표본의 '표본분산' 을 사용 해야한다. \n",
    "\n",
    "$\\sigma^2$ 은 오차항 $\\epsilon = Y-\\beta_0 - \\beta_1 x$ 의 분산인데 , $\\beta_0$ 과 $\\beta_1$ 은 관측되지 않으므로 <br>\n",
    "최소제곱법으로 추정된 $\\hat \\beta_0$ 와 $\\hat \\beta_1$을 대입하여 \n",
    "### $s^2 = \\frac{1}{n-2} \\sum_{i=1}^n (Y_i - \\hat \\beta_0 - \\hat \\beta_1 x_i)^2$ \n",
    "으로 추정할 수 있다. <br>\n",
    "* 이때, 2개의 회귀계수를 추정하고자 하였기에 자유도 2를 잃어 자유도 $df=n-2$ 이다. (이렇게 바꿔서 추정을 해서 분포는 Normal 에서 t분포로 바뀌게 된다.)\n",
    "\n",
    "* $s^2$ 은 평균제곱오차 (mean square error, MSE)라고도 하는데, 이는 단순선형회귀모형의 가정하에서 $\\sigma^2$의 비편향추정량임이 알려져 있다.\n",
    "\n",
    "least square를 통해 구한 계수의 표본분산은 다음과 같다. (이는 '오차의 분산이 같다' 라는 가정이 있어야만 성립한다.)<br>\n",
    "\n",
    "#### **[$\\beta_0$ 와 $\\beta_1$ 에 대한 추정과 검정]**\n",
    "최소제곱법을 통한 회귀계수의 추정이나 추정된 회귀식을 이용한 예측값이 계산에서는 오차항의 분포에 대한 가정이 필요하지 않았다. <br>\n",
    "하지만 회귀계수에 대한 '구간추정'이나 '검정'을 위해서는 오차항의 분포에 대한 가정이 필요하며, 가장 보편적인 것이 **정규성가정** 이다. <br>\n",
    "\n",
    "오차의 **정규성**과 오차항의 **독립성**을 가정하고 단순선형회귀와 관련된 추론을 해보면 아래와 같은 신뢰구간을 구할 수 있다.\n",
    "### $\\hat \\beta_1 \\pm t_\\frac{\\alpha}{2} (n-2) \\frac{s}{\\sqrt{\\sum(x_i-\\bar x_n)^2}}$\n",
    "### $\\hat \\beta_0 \\pm t_\\frac{\\alpha}{2} (n-2) s \\sqrt{\\frac{1}{n} + \\frac{\\bar x_n^2}{\\sum (x_i - \\bar x_n)^2}}$\n",
    "\n",
    "통계량은 \n",
    "### $T = \\frac{\\hat \\beta_1 - 0}{s / \\sqrt{\\sum(x_i - \\bar x_n)^2}}$\n",
    "\n",
    "추정량 $\\beta_0$ 와 $\\beta_1$ 에대한 결과\n",
    "### $SE(\\hat \\beta_0)^2$ = $\\sigma^2 [\\frac{1}{n} + \\frac{\\bar x_n ^2}{\\sum_{i=1}^n (x_i - \\bar x)^2}]$\n",
    "### $SE(\\hat \\beta_1)^2$ = $\\frac{\\sigma^2}{\\sum_{i=1}^n (x_i - \\bar x)^2}$\n",
    "\n",
    "식을 보면 알겠지만, $\\sum_{i=1}^n (x_i - \\bar x)^2$ 이 클수록 즉 x의 변동이 클수록, 즉 x들이 멀리 퍼져있을 수록 $\\hat \\beta_1$의 SE도 확실하게 낮아지는데, <br>\n",
    "직관적으로 데이터가 $\\bar x$ 에서 멀리 퍼져있으면 기울기를 예측하는데 더 도움이 될것이라고 이해가 가능하다.\n",
    "\n",
    "#### **[회귀식에 대한 추정과 검정]**\n",
    "단순선형회귀식에서 기울기에 해당하는 $\\beta_1$은 설명변수의 값이 변할 때 반응변수가 어떻게 변하는지를 말해주는 수치로서, <br>\n",
    "이 값이 0이라면 설명변수와 반응변수 사이에 아무런 관계가 없는 것이 된다. <br>\n",
    "그러므로 이 값이 0과 유의하게 다른지를 자료로부터 밝히는 것이 중요하다. <br>\n",
    "\n",
    "* 설명변수와 반응변수 사이에 유의한 선형적인 관련성이 있는지를 확인하기 위해서 귀무가설 $H_0: \\beta_1 = 0$ 을 검정해보자. \n",
    "    * 대립가설은 $H_1 : \\beta_1 >0 , \\beta_1 <0 $, 또는 $\\beta_1 \\ne 0$ 로 놓을 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------\n",
    "#### **[$E(Y)$에 관한 추정과 검정]**\n",
    "x의 값이 주어졌을 때, Y의 조건부 기댓값 E(Y), 즉 $\\beta_0 + \\beta_1 x$ 에 대한 추론을 생각해보자. <br>\n",
    "### $E(Y) = \\beta_0 + \\beta_1 x$ \n",
    "의 추정량은 $\\hat Y = \\hat \\beta_0 + \\hat \\beta_1 x$ 이다. <br>\n",
    "이때 $E(\\beta_0 + \\beta_1 x)$ = $\\beta_0 + \\beta_1 x$  이므로, <br>\n",
    "$\\hat \\beta_0 + \\hat \\beta_1 x$  는 $E(Y) = \\beta_0 + \\beta_1 x$ 의 불편추정량이다.  <br>\n",
    "\n",
    "\n",
    "### $Var(\\hat Y)$ = $Var(\\beta_0 + \\beta_1 x)$ = $Var(\\bar Y + \\hat \\beta_1(x-\\bar x))$\n",
    "### $= \\frac{\\sigma^2}{n} + (x-\\bar x)\\frac{\\sigma^2}{S_(xx)}$\n",
    "\n",
    "또한 $\\beta_0 + \\beta_1 x$ 는 서로 독립이며 정규분포를 따르는 확률변수 $Y_1,Y_2,,,,Y_n$ 의 일차결합이므로 마찬가지로 정규분포를 따른다. <br>\n",
    "따라서 아래식이 성립한다.\n",
    "### $\\hat \\beta_0 + \\hat \\beta_1 x$ ~ $N(\\beta_0 + \\beta_1 x), [\\frac{1}{n}+\\frac{(x-\\bar x)^2}{\\sum(x_i-\\bar x_n)^2}]\\sigma^2$\n",
    "\n",
    "신뢰구간의 길이 <br>\n",
    "### $2t_\\frac{\\alpha}{2} (n-2)s \\sqrt{\\frac{1}{n} + \\frac{(x-\\bar x)^2}{\\sum(x_i-\\bar x_n)^2}}$\n",
    "\n",
    "=> 신뢰구간은 가능하면 그 길이가 짧은 것이 좋으므로 $\\sum(x_i-\\bar x_n)^2$ 의 값이 크도록 $x_i$들의 위치를 잡아서 자료를 취해주는 것이 바람직하다. <br>\n",
    "또한 이 신뢰구간의 길이는 n이 커지면 짧아진다. <br>\n",
    "이는 n이 커질수록 $E(Y)$의 위치를 더 정확히 구할 수 있다는 의미이다. <br>\n",
    "(참고) $x = \\bar x$ 일때, 신뢰구간이 가장 짧은것을 알 수 있다.\n",
    "\n",
    "#### **[예측구간과 신뢰구간 차이]**\n",
    "* 신뢰구간은 각 x값에 대한 y의 '평균' 범위 구간을 의미한다.\n",
    "* 예측 구간은 각 x값에 대해서 예측되는 y값의 범위를 의미한다. <br>\n",
    "---> 이러한 이유로 예측구간은 항상 신뢰구간보다 더 넓다.\n",
    "\n",
    "1) 주어진 $x$ 에 대한 $E(y)=\\beta_0 + \\beta_1x$ 의 평균추정 <br>\n",
    "### $\\hat \\beta_0 + \\hat \\beta_1 x \\pm t_\\frac{\\alpha}{2}(n-2) s \\sqrt{\\frac{1}{n} + \\frac{(x-\\bar x_n)^2}{\\sum(x_i-\\bar x_n)^2}}$\n",
    "\n",
    "2) 주어진 $x$ 에 대한 $y = \\beta_0 + \\beta_1 x + \\epsilon$의 예측 <br>\n",
    "### $\\hat \\beta_0 + \\hat \\beta_1 x \\pm t_\\frac{\\alpha}{2}(n-2) s \\sqrt{1+ \\frac{1}{n} + \\frac{(x*-\\bar x_n)^2}{\\sum(x_i-\\bar x_n)^2}}$\n",
    "\n",
    "<img src=\"regression_image.PNG\" width=\"700\"> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------\n",
    "#### **[단순회귀의 분산분석]**\n",
    "=> 최소제곱회귀직선을 구하여 사용하는 것이 의미가 있는가를 판단하는 데에 분산분석이 쓰인다. <br>\n",
    "\n",
    "주어진 자료에 대해서 모형을 추정한 후에는 추정되니 모형의 설명력에 관심을 가지게 된다. <br>\n",
    "회귀모형이 반응변수의 변동을 얼마나 설명할 수 있는지 알아보는 것이 중요. <br>\n",
    "\n",
    "반응 변수 Y의 총 변동은 평균에서부터 편차를 이용하여 전체 제곱합<br>\n",
    "#### $SST = \\sum_{i=1}^n (Y_i - \\bar Y)^2$\n",
    "으로 측정할 수 있다. 이는 regression을 하기도 전에 있던 , 전체의 평균에서 각 값들이 얼마나 왔다갔다 하는가를 의미하는 지표이다.  <br>\n",
    "\n",
    "잔차 제곱합은 \n",
    "#### $SSE = \\sum_{i=1}^n (Y_i - \\hat Y_i)^2$\n",
    "으로 나타내고 , 이는 우리가 추정된 함수선이 예측하지 못한 변동을 말한다. <br>\n",
    "\n",
    "회귀제곱합은 \n",
    "#### $SSR = \\sum_{i=1}^n (\\hat Y_i - \\bar Y)^2$ \n",
    "으로 나타내고, 우리가 예측한 '왔다갔다 하는 정도' 즉, 데이터의 변동은 우리의 추정된 함수선\n",
    "\n",
    "이는 다음과 같은 관계가 성립한다. <br>\n",
    "$SST = SSE + SSR$ <br>\n",
    "$\\sum_{i=1}^n (Y_i - \\bar Y)^2$ = $\\sum_{i=1}^n (Y_i - \\hat Y_i)^2$ + $\\sum_{i=1}^n (\\hat Y_i - \\bar Y)^2$\n",
    "\n",
    "\n",
    "여기서 회귀제곱합을 전체제곱합으로 나눈 것을 **결정계수(coefficient of determination)** 라고 하고 $R^2$ 로 표시한다.\n",
    "### $R^2 = \\frac{SSR}{SST}$ $= \\frac{\\sum_{i=1}^n (\\hat Y_i - \\bar Y)^2}{\\sum_{i=1}^n (Y_i - \\bar Y)^2}$ = $1 - \\frac{SSE}{SST}$\n",
    "\n",
    "* 결정계수는 반응변수 Y의 변동 중에 x에 대한 회귀식으로 설명되는 변동의 비율을 나타내며, 이 값이 크면 두 변수 간의 선형관계가 강하다고 할 수 있다.\n",
    "* ex) 만약 결정계수가 0.35이면 Y의 변동 중 35%는 회귀모형으로 설명되고 나머지 65%는 모형에 포함하지 않은 다른 요인들로 설명된다는 뜻이다.\n",
    "* 이러한 결정계수는 단순선형회귀에서는 반응변수와 설명변수 간 상관계수의 제곱과 정확히 일치하는데, 상관계수가 두 수 사이의 선형관계의 강도를 측정한다는 점에서 타당한 통계적 의미를 지닌다.\n",
    "    * 즉, 단순선형회귀에 국한해서 $R^2 = r^2$이다.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------\n",
    "#### [회귀직선의 유의성 검정]\n",
    "단순회귀모형 $Y_i = \\beta_0 + \\beta_1 x + \\epsilon$ 에서 가설 <br>\n",
    "$H_0 : \\beta_1 = 0, H_1 : \\beta_1 \\ne 0$ 에 대한 검정통계량은 아래와 같다.<br>\n",
    "### $F = \\frac{MSR}{MSE}$ ~ $F(1,n-2)$\n",
    "=> 회귀로도 좁힐 수 없는 예측 오차 대비, 회귀로 인해 좁혀진 예측 오차가 얼마나 큰가를 표현한 수치가 F-통계량이다.\n",
    "\n",
    "< 증명 > <br>\n",
    "### $\\frac{\\hat \\beta_1 - \\beta_1}{\\sqrt{\\frac{MSE}{S_(xx)}}}$ ~ $t(n-2)$\n",
    "이므로, t분포와 F분포의관계에서 (위의 분모 분자를 각각 제곱)<br>\n",
    "### $\\frac{(\\hat \\beta_1 - \\beta_1)^2}{MSE}$ ~ $F (1, n-2)$\n",
    "임을 알 수 있다. 그리고 $MSR = \\hat \\beta_1^2 S_(xx)$ 이므로 $\\beta_1 = 0$ 일때에는\n",
    "$\\frac{MSR}{MSE}$ ~ $F(1,n-2)$\n",
    "\n",
    "이 F분포의 자유도는 회귀제곱합의 자유도(1) 와 잔차제곱합의 자유도(n-2)이다. <br>\n",
    "만약 F(1,n-2) 의 분포에 비해 관측된 F값이 매우 크다면 이는 귀무가설에서 기대되는 것보다 평균회귀제곱(MSR)이 상대적으로 크다는 뜻이므로, <br>\n",
    "F값이 클 때 회귀모형이 ㅠㅇ의하지 않다는 귀무가설을 기각하고, <br>\n",
    "반응변수와 설명변수 사이에 선형관계가 유의하다고 결론을 내리게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "suhyun2",
   "language": "python",
   "name": "suhyun2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
